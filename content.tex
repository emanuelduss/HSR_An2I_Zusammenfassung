%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Integralrechnung}
\subsection{Grundlagen}
\subsubsection{Bestimmtes Integral}
Sei $f$ eine auf dem Intervall $[a;b]$ definierte Funktion. Wenn der
Grenzwert
\[ \int^b_a f = \int^b _a f(x) dx =
  \lim_{n \to \infty} \sum_{k=1}^{n}f(x_k) \cdot \Delta x
\]
mit 
\[ \Delta x = \frac{b-a}{n} \text{ und } x_k = a + k \cdot \Delta x\]
existiert, dann heisst die Funktion auf dem Intervall $[a;b]$
integrierbar.

\subsubsection{Graphische Interpretation von Integralen}
Beim Integral $\int_{a}^{b}f$ gibt es zwei Fälle:
\begin{itemize}
  \item Wenn $a < b$: Positive Ordinate zählt positiv;
    Negative Ordinate zählt negativ
  \item Wenn $a > b$: Positive Ordinate zählt negativ;
    Negative Ordinate zählt positiv
\end{itemize}

\subsubsection{Grundregeln für Integrale}
\begin{itemize}
  \item Faktorregel: $\int_{a}^{b}c \cdot f = c \cdot \int_{a}^{b}f$
  \item Vertauschen der Integralgrenzen ändert das Vorzeichen des Integrals:
    $\int_{a}^{b}f = - \int_{b}^{a}f$
  \item Aneinanderstossende Integrale können zusammengefasst werden:
    $\int_{a}^{b}f + \int_{b}^{c}f = \int_{a}^{c}f$
  \item Linearität: $\int_{a}^{b}(f+g) = \int_{a}^{b}f + \int_{a}^{b}g$
  \item Gleiche Integrationsgrenzen $\int_{a}^{a} f = 0$
\end{itemize}

\subsubsection{Nummerische Berechnung von Integralen}
Rechteckregel:
\[ \int^b_a f =  \Delta x \sum_{k=1}^{n}f(x_k) \]
mit
\[ \Delta x = \frac{b-a}{n} \text{ und } x_k = a + k \cdot \Delta x \]

\subsection{Berechnung von Integralen mit Stammfunktionen}
\subsubsection{Integralfunktion}
\[ \phi_a(x) = \int_a^x f \]
\begin{itemize}
  \item Die Integralfunktion hängt vom Parameter $a$ ab.
  \item Ändert man den Parameter $a$, so ändert sich die
  Integralfunktion nur um eine Konstante ($\phi_b(x) = \phi_a(x) + c$),
  was eine Verschiebung auf der Y-Achse bewirkt.
\end{itemize}

Aus der Ableitung der Integralfunktion erhalten wir die ursprüngliche Funktion:
\[ \frac{x}{dx} \phi_a(x) = \frac{dx}{x} \int_a^x f = f(x) \]

\subsubsection{Stammfunktion}
Wir nennen eine Funktion $F$ Stammfunktion von $f$, wenn die Ableitung
der Stammfunktion $f$ ergibt:
\[F \text{ mit } F' = f\]

\subsubsection{Hauptsatz}
Jede Integralfunktion ist eine Stammfunktion. Umgekehrt kann jede
Stammfunktion zum berechnen von Integralen benutzt werden.
\[\phi_a(b) = \int_a^b f(x)dx = F(x)|^b_{x=a} = F(x)|^b_a = F(b) - F(a) \]

\subsubsection{Unbestimmtes Integral}
Das unbestimmte Integral $\int f$ ist die Menge aller Stammfunktionen
von $f$.
\[ \int f(x) dx = F(x) + c \text{ bzw. } \int f = F + c\]

\subsubsection{Rechenregeln}
\begin{itemize}
  \item Verkettung linearer Funktionen: $\int f(ax+b)dx = \frac{1}{a}F(ax+b) + c$
  \item Produkteregel: $\int_a^b f'(x) \cdot g(x) dx =
    (f(x) \cdot g(x))|^b_a - \int_a^b f(x) \cdot g'(x) dx$
  \item Spezialfall der Produkteregel: $\int f(x) \cdot f'(x) dx = \frac{1}{2}f^2(x) + c$
  \item Quotientenregel: $\int \frac{f'(x)}{f(x)}dx = ln(|f(x)|) + c$
  \item Substitutionsregel: $ \int f(g(x)) \cdot g'(x) dx = F(g(x)) + c$
\end{itemize}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fouriertransformation}
\subsection{Fourierreihen}
Eine Fourierreihe der Funktion $f$ besteht aus einer Linearkombination
von Sinus- und Kosinus-Funktionen, welche alle dieselbe Persiode $T$
haben. Je höher die Ordnung $n$, desgo genauer wird die Funktion $f$ aproximiert.

\subsubsection{Sinus-Kosinus-Form}
\[
  f(t) = a_0 + \sum_{k=1}^{n}
  (a_k \cdot cos(k \omega_1 t) + b_k \cdot sin(k \omega_1 t))
\]
\begin{itemize}
  \item Grundkreisfrequenz (Gemeinsame Periode) $\omega_1 = \frac{2\pi}{T}$
  \item Konstante $a_0 = A_0 = \frac{1}{T}\int_0^T s(t) dt$
  \item Koeffizient $a_k = A_k \cdot cos(\phi_k) =
    \frac{2}{T} \int_0^T s(t) \cdot cos(k \omega_1 t) dt$
  \item Koeffizient $b_k = A_k \cdot sin(\phi_k) =
    \frac{2}{T} \int_0^T s(t) \cdot sin(k \omega_1 t) dt$
\end{itemize}

\subsubsection{Amplituden-Phasen-Form}
\[
  f(t) = A_0 + \sum_{k=1}^{n}
  (A_k \cdot cos(k \omega_1 t  - \phi_k))
\]
\begin{itemize}
  \item Grundkreisfrequenz (Gemeinsame Periode) $\omega_1 = \frac{2\pi}{T}$
  \item Konstante $A_0 = a_0$
  \item Koeffizient $A_k = \sqrt{a_k^2 + b_k^2}$
  \item $\phi_k =  \begin{cases}
      arctan\left(\frac{b_k}{a_k}\right) & \text{ für } a_k > 0 \\
      arctan\left(\frac{b_k}{a_k}\right) + \pi & \text{ für } a_k < 0 \\
      \frac{\pi}{2} & \text{ für } a_k = 0 \wedge b_k > 0\\
      -\frac{\pi}{2} & \text{ für } a_k = 0 \wedge b_k < 0\\
    \end{cases}$
\end{itemize}

\subsection{Eigenschaften}
\subsubsection{Gerade und ungerade Funktionen}
\paragraph{Gerade Funktionen (Reine Kosinusreihe)}
\begin{itemize}
  \item Für die geraden Funktionen ist das Integral
    $\int_0^{\frac{T}{2}}$ am optimalsten.
  \item Sinus-Kosinus-Form: $s(t) = a_0 + \sum_{k=1}^{n}
    (a_k \cdot cos(k \omega_1 t))$
  \item Koeffizient $a_k = \frac{4}{T} \int_0^{\frac{T}{2}} s(t) \cdot cos(k \omega_1 t) dt$
  \item Koeffizient $b_k = 0$
\end{itemize}
\paragraph{Ungerade Funktionen (Reine Sinusreihe)}
\begin{itemize}
  \item Für die ungeraden Funktionen ist das Integral
    $\int_0^{\frac{T}{2}}$ am optimalsten.
  \item Sinus-Kosinus-Form: $f(t) = a_0 + \sum_{k=1}^{n}
    (b_k \cdot sin(k \omega_1 t))$
  \item Koeffizient $b_k = \frac{4}{T} \int_0^{\frac{T}{2}} s(t) \cdot sin(k \omega_1 t) dt$
  \item Koeffizient $a_k = 0$
\end{itemize}

\subsubsection{Transformation von Fourierreihen}
\paragraph{Spiegeln an X-Achse}
\begin{itemize}
  \item Transformation: $r(t) = -s(t)$
  \item Sinus-Kosinus-Form: $\boldsymbol{-a_0} + \sum_{k=1}^{n}
    (\boldsymbol{-a_k} \cdot cos(k \omega_1 t) +
    \boldsymbol{(-b_k)} \cdot sin(k \omega_1 t))$
  \item Amplituden-Phasen-Form: $\boldsymbol{-A_0} + \sum_{k=1}^{n}
    (A_k \cdot cos(k \omega_1 t  - \boldsymbol{(\phi_k + \pi)}))$
\end{itemize}
\paragraph{Spiegeln an Y-Achse}
\begin{itemize}
  \item Transformation: $r(t) = s(-t)$
  \item Sinus-Kosinus-Form: $a_0 + \sum_{k=1}^{n} ( a_k \cdot
    cos(k \omega_1 t) + \boldsymbol{(-b_k)} \cdot sin(k \omega_1 t))$
  \item Amplituden-Phasen-Form: $A_0 + \sum_{k=1}^{n}
    (A_k \cdot cos(k \omega_1 t  - \boldsymbol{(-\phi_k)}))$
\end{itemize}
\paragraph{Skalierung auf der X-Achse (Zeitskalierung)}
\begin{itemize}
  \item Transformation: $r(t) = s(c \cdot t)$ für $c > 0$
  \item Für $c < 0$: Zusätzliche Transformation: $r(t) = s(-t)$
  \item Periodendauer $\hat{T} = \frac{1}{c}T$
  \item Sinus-Kosinus-Form: $a_0 + \sum_{k=1}^{n}
    a_k \cdot cos(k \boldsymbol{(c \omega_1)} t) +
    b_k \cdot sin(k \boldsymbol{(c \omega_1)} t))$
  \item Amplituden-Phasen-Form: $A_0 + \sum_{k=1}^{n}
    (A_k \cdot cos(k \boldsymbol{(c \omega_1)} t  - \phi_k))$
\end{itemize}
\paragraph{Skalierung auf der Y-Achse (Vertikalskalierung)}
\begin{itemize}
  \item Transformation: $r(t) = c \cdot s(t)$ für $c > 0$
  \item Sinus-Kosinus-Form: $\boldsymbol{(c \cdot a_0)} + \sum_{k=1}^{n}
  ( \boldsymbol{(c \cdot a_k)} \cdot cos(k \omega_1 t) + \boldsymbol{(c
  \cdot b_k)} \cdot sin(k \omega_1 t))$
  \item Amplituden-Phasen-Form: $\boldsymbol{(c \cdot A_0)} + \sum_{k=1}^{n}
    (\boldsymbol{(c \cdot A_k)} \cdot cos(k \omega_1 t  - \phi_k))$
\end{itemize}
\paragraph{Verschieben auf der X-Achse (Zeitverschiebung)}
\begin{itemize}
  \item $r(t) = s(t-c)$ für $c > 0$
  \item Sinus-Kosinus-Form mittels Additionstheoremen berechnen
    \begin{itemize}
      \item[=] $a_0 + \sum_{k=1}^{n} (a_k \cdot cos(k \omega_1
        \boldsymbol{(t-c)}) + b_k \cdot sin(k \omega_1 \boldsymbol{(t-c)}))$
      \item[=] $a_0 + \sum_{k=1}^{n}(
        a_k \cdot cos(\boldsymbol{k \omega_1 t - k \omega_1 c}) +
        b_k \cdot sin(\boldsymbol{k \omega_1 t - k \omega_1 c}))$
      \item[=] $a_0 + \sum_{k=1}^{n}[
        a_k (cos(k \omega_1 t) \cdot cos(k \omega_1 c) +
        sin(k \omega_1 t) \cdot sin(k \omega_1 c)) + \\
        b_k (sin(k \omega_1 t) \cdot cos(k \omega_1 c) -
        sin(k \omega_1 t) \cdot cos(k \omega_1 c))$
      \item[=] $a_0 + \sum_{k=1}^{n} [
        \boldsymbol{(a_k \cdot cos(k \omega_1 c) -
        b_k \cdot sin(k \omega_1 c))} \cdot cos(k \omega_1 t) + \\
        \boldsymbol{(a_k \cdot sin(k \omega_1 c) +
        b_k \cdot cos(k \omega_1 c))} \cdot sin(k \omega_1 t)]$
    \end{itemize}
  \item Amplituden-Phasen-Form: $A_0 + \sum_{k=1}^{n}
    (A_k \cdot cos(k \omega_1 t - \boldsymbol{(k \omega_1 c + \phi_k)}))$
  % \item Sinus-Kosinus-Form: $f(t) = a_0 + \sum_{k=1}^{n} (a_k \cdot cos(k \omega_1 t) + b_k \cdot sin(k \omega_1 t))$
\end{itemize}
\paragraph{Verschieben auf der Y-Achse (Vertikalverschiebung)}
\begin{itemize}
  \item Transformation: $r(t) = s(t) + c$
  \item Sinus-Kosinus-Form: $\boldsymbol{(a_0 + c)} + \sum_{k=1}^{n} ( a_k \cdot
    cos(k \omega_1 t) + b_k \cdot sin(k \omega_1 t))$
  \item Amplituden-Phasen-Form: $\boldsymbol{(A_0 + c)} + \sum_{k=1}^{n}
    (A_k \cdot cos(k \omega_1 t  - \phi_k))$
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Differenzialgleichungen}
\subsection{Begirffe}
\begin{itemize}
  \item Anwendungsgebiet: Viele Naturgesetze können mit einer
    Differenzialgleichung (DGL) modelliert werden.
  \item Differenzialgleichung: Gleichung zwischen einer Funktion und
    derer Ableitung
  \item Ordnung: Höchste vorkommende Ableitung
  \item Allgemeine Lösung: Lösungsmenge mit unendlich verschiedenen
    Lösungen (eine Differenzialgleichung hat in der Regel unendlich viele Lösungen)
  \item Spezielle Lösung: Einzelne Lösung aus der Lösungsmenge
  \item Anfangsbedingungen: Bedingungen um spezielle Lösung anzugeben.
    Vorgabe des Funktionswerts und aller Beleitungen der Funktion bis zum
    Grad Ordnung - 1 an einer einzigen gemeinsamen Stelle (Z. B. 1.
    Ordnung: $f(x_0) = 23$). Die Anfangsbedingungen sorgen dafür, dass
    die DGL nur noch von einer einzigen speziellen Lösung erfüllt
    werden.
  \item Anfangswertproblem: Differenzialgleichung mit Anfangsbedingungen
\end{itemize}

\subsubsection{Explizite Differenzialgleichung}
Bei einer expliziten Differenzialgleichung ist die höchste Ableitung ist
auf eine Seite isoliert:
\[ \frac{df}{dx} = f'(x) = G(x, f(x)) \]

\subsection{Näherungslösung mit dem Verfahren nach Euler}
Gegeben ist eine Differenzialgleichung:
\[ f'(x) = 2x - 3f(x) + 1 \]
Mit dem \textcolor{red}{Anfangswertproblem}:
\[ f(0) = 0 \]

\begin{small}
\begin{tabular}{|l|l|l|l|l|}
  \hline
    Stufe & x & Funktion &
      Ableitung &
      Linearisierung \\
    \# & $x$ & $f(x)$ &
      $f'(x) = 2x - 3f(x) + 1$ &
      $f(\widetilde{x}) = f(x) + f'(x)(\widetilde{x}-x)$ \\
  \hline \hline
    0 & \textcolor{red}{$0$} & \textcolor{red}{$f(0) = 0$} &
      $f'(x) = 2 \cdot 0 - 3 \cdot 0 + 1 = 1 $ &
      $f(\widetilde{x}) = 0 + 1(\widetilde{x} - 0)$ \\
  \hline
    1 & 0.1 & $f(\widetilde{x}) = 0 + 1(0.1 -0) = 0.1$ &
      $2 \cdot 0.1 - 3 \cdot 0.1 + 1 = 0.9$ &
      $0.1 + 0.9(\widetilde{x} - 0.1)$ \\
  \hline
    2 & 0.2 & $0.1 + 0.9(0.2 - 0.1) = 0.19$ &
      $2 \cdot 0.2 - 3 \cdot 0.19 + 1 = 0.83$ &
      $0.19 + 0.83(\widetilde{x} - 0.2)$ \\
  \hline
    3 & 0.3 & $0.19 + 0.83(0.3 - 0.2) = 0.273$ &
      $\dots$ &
      $\dots$ \\
  \hline
\end{tabular}
\end{small}

Somit ist $f(0.3) \approx 0.273$.

\subsection{Separierbare Differenzialgleichung}
Eine separierbare Differenzialgleichung kann auf folgende Form gebracht
werden:
\[ f'(x) = \frac{g(x)}{h(f(x))} \]
Es handelt sich um eine explizite Differenzialgleichung erster Ordnung,
wobei die Ableitung $f'(x)$ der gesuchten Funktion als Quotient dargestellt
wird. Der Zähler ist abhängig von $x$, der Nenner von $f(x)$.

Oder als Produkt:
\[ f'(x) = g(x) \cdot h(f(x)) \]
Ein Faktor ist nur von $x$, der andere nur von $f(x)$ abhängig.

\subsubsection{Lösen von separierbaren Differenzialgleichungen}
Schritt 0: Schreibweise: Die Differenzialgleichungen wird in die
Termschreibweise gebracht.
\[ \frac{df}{dx} = g(x) \cdot h(f) \]

\paragraph{Schritt 1:} Separieren: Wir behandeln $df$ und $dx$ wie Variabeln mit dem Ziel $df$ und $f$
links, $dx$ und $x$ rechts vom Gleichheitszeichen zu haben:
\[ \frac{1}{h(f)} df = g(x) dx \]

\paragraph{Schritt 2:} Integrieren: Danach wird auf beiden Seiten integriert:
\[ \int \frac{1}{h(f)} df = \int g(x) dx \]

\paragraph{Schritt 3:} Auflösen: Danach kann die Gleichung nach $f$
aufgelöst werden. Die Integrationskonstante darf nicht vergessen werden,
weil man sonst nicht die allgemeine Lösung erhält.

\subsection{Lineare Differenzialgleichungen mit konstanten Koeffizienten}
Eine lineare Differenzialgleichung mit konstanten Koeffizienten hat die Form
\[ a_{0}f(x) + a_{1}f'(x) + a_{2}f''(x) + \dots + a_{n}f^{(n)}(x) = g(x) \]
oder
\[ \sum_{k=0}^{n} a_{k}f^{(k)}(x) = g(x) \]
\begin{itemize}
  \item $a_k$: Gegebene Konstanten
  \item $g$: Vorgegebene Funktion (Störfunktion): $g = 0$: homogen; $g \ne 0$: inomogen
\end{itemize}

\subsubsection{Linearitätsgesetze der homogenen linearen Differenzialgleichung}
\begin{itemize}
  \item Sind $f_1$ und $f_2$ Lösungen einer homogenen linearen
    Differenzialgleichung, ist auch die Summe davon eine Lösung.
  \item Ist $f$ eine Lösung einer homogenen linearen
    Differenzialgleichung und $c$ eine Konstante, ist auch das Produkt
    eine Lösung.
\end{itemize}

\subsubsection{Homogene lineare Differenzialgleichung 1. Ordnung}
Die lineare Differenzialgleichung 1. Ordnung
\[ 2f(x) + 3f'(x) = 0 \]
kann mit diesem Ansatz gelöst werde, indem man $f(x)$ und $f'(x)$
ersetzt:
\[ f(x) = e^{sx} \] und
\[ f'(x) = s \cdot e^{sx} \]
Datei ist $s$ eine noch unbekannte Konstante. Das setzt man in die
Differenzialgleichung ein:
\[ 2 \cdot e^{sx} + 3 \cdot s \cdot e^{sx} = 0 \]
Dann wird durch $e^{sx}$ dividiert, damit man die Konstante $s$ erhält:
\[ 2 + 3 \cdot s = 0 \Leftrightarrow s = - \frac{2}{3} \]
Somit ist eine Lösung (spezielle Lösung):
\[f(x) = e^{-\frac{2}{3}x} \]
Und aufgrund des Linearitätsgesetzes auch jedes konstante Vielfache
davon (allgemeine Lösung):
\[f(x) = A \cdot e^{-\frac{2}{3}x} \]

\subsubsection{Homogene lineare Differenzialgleichung 2. Ordnung}
Eine lineare Differenzialgleichung 2. Ordnung ist nie Separierbar.

\paragraph{Variante 1:} Das gleiche Verfahren kann auch bei homogenen
linearen Differenzialgleichungen 2. Ordnung angewendet werden:
\[ f''(x) + f'(x) - 6f(x) = 0 \]
Ersetzen:
\[ f(x) = e^{sx} \Rightarrow f'(x) = s \cdot e^{sx}
  \text{ und } f''(x) = s^2 \cdot e^{sx} \]
Daraus ergibt sich die Gleichung:
\[ s^2 \cdot e^{sx} + s \cdot e^{sx} - 6 \cdot e^{sx} = 0 \]
Nach dem Dividieren durch $e^{sx}$ erhält man
\[ s = \frac{-1 \pm 5}{2} \]
Daraus ergeben sich folgende zwei Lösungen:
\[ f(x) = e^{2x} \text{ und } f(x) = e^{-3x} \]
Die Allgemeine Lösung lautet deshalb:
\[ f(x) = A \cdot e^{2x} + B \cdot e^{-3x} \]

Nicht jede homogene lineare Differenzialgleichung 2. Ordnung muss eine
Lösung in der Form besitzen.

\paragraph{Variante 2:}
Es gibt einen Lösungsansatz via charakteristisches Polynom.
\[ f''(x) + f'(x) - 6f(x) = 0 \]
Daraus kann das charakteristische Polynom gebildet werden (die
Nullstellen helfen uns die Lösung zu finden):
\[ 1 \cdot a^2 + 1 \cdot a - 6 = 0 \]
Wenn man das charakteristische Polynom nach $a$ auflöst, erhält man wie beim vorherigen Beispiel:
\[ a = \frac{-1 \pm 5}{2} \]
Durch den Ansatz $f(x) = e^{ax}$, $f'(x) = ae^{ax}$ und $f''(x) =
a^{2}e^{ax}$ erhält man die allgemeine Lösung:
\[ f(x) = A \cdot e^{2x} + B \cdot e^{-3x} \]

\subsubsection{Allgemein: Homogene lineare Differenzialgleichung 2. Ordnung}
Homogene lineare Differenzialgleichung 2. Ordnung:
\[ c_2f''(x) + c_1f'(x) + c_0f(x) = 0 \]
In charakteristisches Polynom umwandeln:
\[ c_2s^2 + c_1s + c_0 = 0 \]
\paragraph{Fall 1:} Zwei verschiedene Lösungen (Diskriminante ist positiv)
\[ f(x) = Ae^{s_1x} + Be^{s_2x} \]
\paragraph{Fall 2:} Nur eine Lösung (Diskriminante ist Null)
\[ f(x) = (Ax + B) e^{s_1x} \]
\paragraph{Fall 3:} Keine Lösung (Diskriminante ist negativ)
\[ f(x) = e^{rx} (A\cos(\omega x) + B (sin(\omega x)) \Leftrightarrow
  f(x) = Ce^{rx} \cos(\omega x - \phi) \]
Lösungsformel auf folgende Form bringen:
\[ s = a \pm \sqrt{b} \]
Dann ist
\[ r = a \text{ und } \omega = \sqrt{-b} \]
\paragraph{Hinweis:} $A$, $B$, $C$ und $\phi$ sind jeweils frei wählbar.

\subsubsection{Inhomogene lineare Differenzialgleichung}
Die allgemeine Lösung einer inhomogenen linearen Differenzialgleichung
für
\[ \sum_{k=0}^{n} a_{k}f^{(k)}(x) = s(x) \]
ist die Summe einer speziellen Lösung  und  der allgemeinen
Lösungen der zugehörigen homogenen Gleichung der Form
\[ \sum_{k=0}^{n} a_{k}f^{(k)}(x) = 0 \]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Anhang}
\subsection{Summenformeln}
\[ \begin{aligned}
  \sum_{i=1}^{n}i     & = \frac{n(n+1)}{2} \\
  \sum_{i=1}^{n}i^2   & = \frac{n(n+1)(2n+1)}{6} \\
  \sum_{i=1}^{n}i^3   & = \left(\frac{n(n+1)}{2}\right)^2 \\
\end{aligned} \]
\subsection{Additionstheoreme}
\begin{itemize}
  \item $sin(a \pm b) = sin(a) cos(b) \pm cos(a) sin(b)$
  \item $cos(a \pm b) = cos(a) cos(b) \mp sin(a) sin(b)$
\end{itemize}
